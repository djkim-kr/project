This is a project about delta ML development.
Examples and data we have here is only containing 'water molecule'
Detailed usage is like below

## Delta_ML data generation 
1. Use 'all_method.py'
    All the functions and execution usage can be found in this file
    from line 281, you can indicate the location of 
    xyz_file(contatinig coordinates, energy) & zmat_file(internal coordinates)

    By running this file, you can get total 5 xyz files that you can use for training.
    direct, harmonic, normal, internal, internal(morse)

    All the data file data to generate to these files for water molecule example is already in 'data' folder
    but I will explain how did I get those.

2. 'data' folder
    1. h2o.c1.freq24.int.log & raw.c1.int.log
        'h2o.c1.freq24.int.log' was generated using CCSD(T) calculation with 'GAMESS'.
        This contains lots of info including hessian matrix, normal modes, etc. 
        Extracted important info is in 'raw.c1.int.log'. 
        And this will be mainly used in 'all_method.py' to get info that we need
    2. 'wm_combine.xyz',
        This is basically all the water molecule data that we have.
        This includes xyz coordinate, energy, forces. I believe they are also from CCSD(T) level.
        We are going to generate ML training data based on this file

        zmat file(internal coordinates) can be converted using 'xyz_to_zmat.py'

    3. 'ang_*.xyz', 'dis_*.xyz'
        These are the files that we need for calculating the 'internal' method.
        These include the energy data for when we only stretch single internal method.
        e.g.) ang_1.xyz contains small number of data only changing angles from certain minimum angle to maximum angle
        This can give an info about how angle changes can affect the energy
        These data were generated by 'generate_en_train.py'
        cf) ang_1(2).xyz has more info than ang_1.xyz (covering broader range, but doesn't guarantee better precision for interpolation)
        from my previous test, internal morse was better with (1) while internal was better with (2)

    4. 'wm_*_0.1.xyz'
        These are the files that we need for calculating the 'normal' method.
        These include the energy data for when we only stretch single normal method.
        'generating1,2,3 (0.1).py' can give the coordinates. (need to choose the coeff. for normal modes -1<c<1)
        This can give an info about how each normal mode can affect the energy

    5. 'wm_short.xyz'
        5 configs from the whole data. you can test the files with this example
        first, make a 'zmat' file using 'xyz_to_zmat.py'
        second, run 'all_method.py'


## Training potentials and get the results on cluster. (check Einsten djkim/WM_ML/combine_all)
'combine_all': is basically an archive of all works that I have done so far on Einstein. 
    To see how I conducted the running on Einstein, let's take a look at the folder 'made5_cross_valid'. 
    In the 'data' folder you can basically put data and it will later be divided into train and test sets automatically. 

1. The main script is 'prepare.py'. 
    This will help prepare all the directories to do a job. The very bottom of this script, 
    you will be able to find sections where you can modify parameters and so on. 
    After using 'prepare.py' there will be folders ready. 
2. Then, run with 'QUIP_RUN.sh', and when running is done. 
3. We can collect all the information by executing 'collect_cross.py', this will generate a '.dat' file 
4. and we can use 'plot_data.py' to get a plot.

cf)In 'made5_result', since I had to train on 'made' or 'original' data and test on 'made5' data, scripts are a bit modified to do so. 
Plus, there's a 'test_collect.sh' to collect data at the beginning step and the last step to compare those two. 
This data will be needed to get a 'txt' file for paraview analysis. 


## making_structures
'making_structures' is the folder that I organized all the 'made_structure' from 1 to 7. 
In each folder, you will be able to find how I generated 'made' data 
+ calculated (normal, harmonic, internal) data
'Making_structures_explain.pdf' has detailed conditions how I generated the data.

+ 'coefficient_plot' has txt file to analyze about these 'made' structures


## Paraview analysis
coefficient_plot/made3_made5_complete_final.pvsm
conatins all the processed data analysis.
When you load this 'state' file, you have to choose '( choose file names )' option
and allocate the 'txt file' in the right position

1. load txt file 
2. make table to points
3. transforms to make it possible to see in a moderate scale
4. make it glyph
5. by choosing a coloring in a glyph, we can see the data distribution.

